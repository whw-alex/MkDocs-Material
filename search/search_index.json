{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Homepage","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#codeblock","title":"Codeblock","text":"<pre><code>import tensorflow as tf\ndef function():\n#comments here\n</code></pre>"},{"location":"codes/","title":"codes","text":""},{"location":"codes/#deep-learning","title":"deep learning","text":""},{"location":"codes/#a-very-simple-code-for-training-the-resnet18-from-scratch","title":"a very simple code for training the resnet18 from scratch","text":""},{"location":"codes/#load-the-data","title":"load the data","text":"<pre><code>from torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport os\n## Note that: here we provide a basic solution for loading data and transforming data.\n## You can directly change it if you find something wrong or not good enough.\n## the mean and standard variance of imagenet dataset\n## mean_vals = [0.485, 0.456, 0.406]\n## std_vals = [0.229, 0.224, 0.225]\ndef load_data(data_dir = \"../hw2_dataset_2022/\",input_size = 224,batch_size = 36):\ndata_transforms = {\n'train': transforms.Compose([\ntransforms.RandomResizedCrop(input_size),\ntransforms.RandomHorizontalFlip(),\ntransforms.ToTensor(),\ntransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n'test': transforms.Compose([\ntransforms.Resize(input_size),\ntransforms.CenterCrop(input_size),\ntransforms.ToTensor(),\ntransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n'train_augmented': transforms.Compose([\ntransforms.RandomResizedCrop(input_size),\ntransforms.RandomRotation(degrees=55),\n# transforms.RandomRotation([90, 180]),\ntransforms.RandomHorizontalFlip(),\n# RandomErasing(),\n# transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\ntransforms.ToTensor(),\ntransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n}\n## The default dir is for the first task of large-scale deep learning\n## For other tasks, you may need to modify the data dir or even rewrite some part of 'data.py'\n## ImageFolder\u5047\u8bbe\u6240\u6709\u7684\u6587\u4ef6\u6309\u6587\u4ef6\u5939\u4fdd\u5b58\uff0c\u6bcf\u4e2a\u6587\u4ef6\u5939\u4e0b\u5b58\u50a8\u540c\u4e00\u4e2a\u7c7b\u522b\u7684\u56fe\u7247\uff0c\u6587\u4ef6\u5939\u540d\u4e3a\u7c7b\u540d\nimage_dataset_train = datasets.ImageFolder(os.path.join(data_dir, '1-Large-Scale', 'train'), data_transforms['train'])\nimage_dataset_valid = datasets.ImageFolder(os.path.join(data_dir,'test'), data_transforms['test'])\ntrain_loader = DataLoader(image_dataset_train, batch_size=batch_size, shuffle=True, num_workers=4)\nvalid_loader = DataLoader(image_dataset_valid, batch_size=batch_size, shuffle=False, num_workers=4)\nreturn train_loader, valid_loader\n</code></pre>"},{"location":"codes/#define-the-models","title":"define the models","text":"<pre><code>from torchvision import models\nimport torch.nn as nn\ndef model_A(num_classes):\nmodel_resnet = models.resnet18(pretrained=False)\nnum_features = model_resnet.fc.in_features\nmodel_resnet.fc = nn.Linear(num_features, num_classes)\nreturn model_resnet\n</code></pre>"},{"location":"codes/#train-and-test","title":"train and test","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport data\nimport models\nimport os\nimport numpy as np\nfrom torch.optim.lr_scheduler import StepLR\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n## Note that: here we provide a basic solution for training and validation.\n## You can directly change it if you find something wrong or not good enough.\ndef train_model(model,train_loader, valid_loader, criterion, optimizer, num_epochs=20):\ndef train(model, train_loader,optimizer,criterion):\nmodel.train(True)\ntotal_loss = 0.0\ntotal_correct = 0\nfor inputs, labels in train_loader:\ninputs = inputs.to(device)\nlabels = labels.to(device)\noptimizer.zero_grad()\nl2_reg = torch.tensor(0.).to(device)\nfor param in model.parameters():\nl2_reg += torch.norm(param, p=2).to(device)\noutputs = model(inputs)\nloss = criterion(outputs, labels) \n_, predictions = torch.max(outputs, 1)\nloss.backward()\noptimizer.step()\ntotal_loss += loss.item() * inputs.size(0)\ntotal_correct += torch.sum(predictions == labels.data)\n# \u66f4\u65b0\u5b66\u4e60\u7387\n# scheduler.step()\n# \u83b7\u53d6\u5f53\u524d\u7684\u5b66\u4e60\u7387\ncurrent_lr = optimizer.param_groups[0]['lr']\nepoch_loss = total_loss / len(train_loader.dataset)\nepoch_acc = total_correct.double() / len(train_loader.dataset)\nreturn epoch_loss, epoch_acc.item(), current_lr\ndef valid(model, valid_loader,criterion):\nmodel.train(False)\ntotal_loss = 0.0\ntotal_correct = 0\nwith torch.no_grad():\nfor inputs, labels in valid_loader:\ninputs = inputs.to(device)\nlabels = labels.to(device)\noutputs = model(inputs)\nloss = criterion(outputs, labels)\n_, predictions = torch.max(outputs, 1)\ntotal_loss += loss.item() * inputs.size(0)\ntotal_correct += torch.sum(predictions == labels.data)\nepoch_loss = total_loss / len(valid_loader.dataset)\nepoch_acc = total_correct.double() / len(valid_loader.dataset)\nreturn epoch_loss, epoch_acc.item()\nbest_acc = 0.0\ntrain_loss_epoch = []\ntrain_acc_epoch = []\nvalid_loss_epoch = []\nvalid_acc_epoch = []\nfor epoch in range(num_epochs):\nprint('epoch:{:d}/{:d}'.format(epoch, num_epochs))\nprint('*' * 100)\ntrain_loss, train_acc, current_lr = train(model, train_loader,optimizer,criterion)\ntrain_loss_epoch.append(train_loss)\ntrain_acc_epoch.append(train_acc)\nprint(\"training: {:.4f}, {:.4f}, lr:{}\".format(train_loss, train_acc, current_lr))\nvalid_loss, valid_acc = valid(model, valid_loader,criterion)\nvalid_loss_epoch.append(valid_loss)\nvalid_acc_epoch.append(valid_acc)\nprint(\"validation: {:.4f}, {:.4f}\".format(valid_loss, valid_acc))\nif valid_acc &gt; best_acc:\nbest_acc = valid_acc\nbest_model = model\ntorch.save(best_model, 'best_model.pt')\nplt.plot(train_loss_epoch)\nplt.ylabel('Train Loss')\nplt.xlabel('Epoch')\nplt.savefig('B-Train Loss.jpg')\nplt.close()\nplt.plot(train_acc_epoch)\nplt.ylabel('Train Acc')\nplt.xlabel('Epoch')\nplt.savefig('B-Train Acc.jpg')\nplt.close()\nplt.plot(valid_loss_epoch)\nplt.ylabel('Test Loss')\nplt.xlabel('Epoch')\nplt.savefig('B-Test Loss.jpg')\nplt.close()\nplt.plot(valid_acc_epoch)\nplt.ylabel('Test Acc')\nplt.xlabel('Epoch')\nplt.savefig('B-Test Acc.jpg')\nplt.close()\ndef test_model(model, valid_loader):\nmodel.train(False)\ntotal_correct = 0\nwith torch.no_grad():\nfor inputs, labels in valid_loader:\ninputs = inputs.to(device)\nlabels = labels.to(device)\noutputs = model(inputs)\n_, predictions = torch.max(outputs, 1)\ntotal_correct += torch.sum(predictions == labels.data)\nepoch_acc = total_correct.double() / len(valid_loader.dataset)\nprint(\"test_acc: {:.4f}\".format(epoch_acc.item()))\ndef plot_tsne(model, valid_loader):\nfc_features = []\ndef hook(model, input, output):\nfc_features.append(input[0].cpu().detach().numpy())\nmodel.fc.register_forward_hook(hook)\nmodel.train(False)\ny = []\nfor inputs, labels in valid_loader:\ny.append(labels.numpy())\ninputs = inputs.to(device)\nlabels = labels.to(device)\noutputs = model(inputs)\nfc_feature = np.concatenate(fc_features)\ny = np.concatenate(y)\ntsne = TSNE(n_components=2)\nfc_embeded = tsne.fit_transform(fc_feature)\nplt.scatter(x=fc_embeded[:, 0], y=fc_embeded[:, 1], c=y, cmap='rainbow', s=1.5)\nplt.savefig('features.jpg')\nif __name__ == '__main__':\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n## about model\nnum_classes = 10\n## about data\ndata_dir = \"../../hw2_dataset_2022\" ## You need to specify the data_dir first\ninput_size = 224\nbatch_size = 36\n## about training\nnum_epochs = 100\nlr = 0.001\n# \u5b9a\u4e49L2\u6b63\u5219\u5316\u53c2\u6570\nl2_lambda = 0.001\n## model initialization\nmodel = models.model_A(num_classes=num_classes)\n#model = models.model_B(num_classes=num_classes)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n## data preparation\ntrain_loader, valid_loader = data.load_data(data_dir=data_dir,input_size=input_size, batch_size=batch_size)\n## optimizer\n# optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\noptimizer = optim.Adam(model.parameters(), lr=lr)\n# optimizer = optim.RMSprop(model.parameters(), lr=lr)\nscheduler = StepLR(optimizer, step_size=30, gamma=0.5)\n# scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n## loss function\ncriterion = nn.CrossEntropyLoss()\ntrain_model(model,train_loader, valid_loader, criterion, optimizer, num_epochs=num_epochs)\nmodel = torch.load('best_model.pt')\ntest_model(model, valid_loader)\n# plot_tsne(model, valid_loader)\n</code></pre>"},{"location":"mistakes/","title":"all the stupid mistakes","text":""},{"location":"mistakes/#data","title":"data","text":""},{"location":"mistakes/#data-copy","title":"data copy","text":"<p>When copying data, copy the zip file. Or, copy it in small folders once a time. If there are too many folders, there might be mistakes, which can be hard to find out. <pre><code>unzip {file name}\n</code></pre></p>"},{"location":"workbench/","title":"Workbench","text":""},{"location":"workbench/#conda","title":"Conda","text":"<pre><code>conda create -n myenv python=3.9 # Create an environment\nconda activate myenv # Enter the environment\nconda install python=3.10 -c &lt;channel&gt; # Install packages\nconda remove python # Remove packages\nconda deactivate # Exit the environment\nconda info --envs # List all envs\nconda remove -n myenv --all # Remove the environment\nconda list\n</code></pre>"},{"location":"workbench/#pip","title":"Pip","text":"<pre><code>python -m pip install packages\n</code></pre>"},{"location":"workbench/#screen","title":"Screen","text":"<pre><code>screen -ls  # list \nscreen -S name # create \ncommand+A D # quit\nscreen -r name # go back\nscreen -X -S name quit # delete\n</code></pre>"},{"location":"workbench/#tmux","title":"tmux","text":"<pre><code>tmux new -s baby # \u521b\u5efa\u65b0\u7684\u4f1a\u8bdda\ncontrol+B D # \u9000\u51fa\u5f53\u524d\u72b6\u6001\ntmux attach -t baby # \u91cd\u65b0\u8fdb\u5165\n</code></pre>"},{"location":"workbench/#errors-and-warnings","title":"Errors and Warnings","text":""},{"location":"workbench/#runtimeerror-cuda-out-of-memory-tried-to-allocate-mib","title":"RuntimeError: CUDA out of memory. Tried to allocate .. MiB","text":"<p>\u5728\u6d4b\u8bd5\u9636\u6bb5\u548cvalid\u9636\u6bb5\u63d2\u5165\u4ee3\u7801<code>with torch.no_grad()</code> <pre><code>def test(model,dataloader):\nmodel.eval()\nwith torch.no_grad(): ###\u63d2\u5728\u6b64\u5904\nfor batch in tqdm(dataloader):\n\u2026\u2026\n</code></pre></p>"}]}